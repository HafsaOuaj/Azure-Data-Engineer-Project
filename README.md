# ğŸ—ï¸ End-to-End Data Engineering Pipeline on Azure with Databricks

This project showcases a full-stack **data engineering pipeline** using **Azure Cloud Services**, **Databricks**, and **Power BI**, built on the **Adventure Works** dataset. It walks through the process from data ingestion to transformation and visualization using modern tools and best practices.

---

## ğŸ“Š Project Overview

![Architecture Diagram](./architecture.png)

- **Dataset**: [Adventure Works (Kaggle)](https://www.kaggle.com/datasets)
- **Pipeline**: Data Factory â Data Lake Gen2 â Databricks â Synapse â Power BI
- **Format**: Raw `.csv` â Transformed `.parquet`

---

## âš™ï¸ Architecture Components

### ğŸ”¹ Data Source
- Public GitHub repository containing AdventureWorks `.csv` files.
- Fetched via HTTP using Azure Data Factory.

### ğŸ”¹ Ingestion (Bronze Layer)
- **Azure Data Factory**: Automated pipeline that stores raw CSVs into Azure Data Lake Gen2 (`bronze/` folder).

### ğŸ”¹ Transformation (Silver Layer)
- **Azure Databricks**: PySpark scripts clean, join, and convert data into optimized Parquet format.
- Transformations include:
  - Adding product names to sales records
  - Removing duplicates
  - Writing with `.write.mode('overwrite')`

### ğŸ”¹ Serving
- **Azure Synapse Analytics**: Connects to the transformed data and enables SQL queries.
- Serverless SQL pool used for fast access.

### ğŸ”¹ Visualization
- **Power BI**: Dashboards connect to Synapse to display sales and product insights interactively.

---

## ğŸ“ Folder Structure (Local Structure)

```bash

ğŸ“ data/
    â”œâ”€â”€ silver/                # Original CSVs
    â”œâ”€â”€ bronze/                # Output of the Databricks transformations as Parquet files
    â””â”€â”€ transformed/           # Output of Azure Synapse Analytics

ğŸ“ notebooks/
    â””â”€â”€ 01_silver_layer.ipynb

ğŸ“ azureSynapseAnalytics/
    â”œâ”€â”€ Create_External_Tables.sql    # Create External Table as an Output of Synapse Analytics      
    â”œâ”€â”€ Create_Schema.sql             # Create Schema     
    â””â”€â”€ Create_View_Gold.sql          # Create Views from DataLake Silver data
    
ğŸ“ reports/
    â””â”€â”€ powerbi_dashboard.pbix

ğŸ“„ README.md
```
## â˜ï¸ Azure Deployment Info

All ETL scripts, notebooks, and pipelines are stored in Azure:

- **Resource Group**: `AWPROJECT`
- **Services used**:
  - Azure Data Factory (pipelines, linked services, datasets)
  - Azure Databricks (notebooks in workspace)
  - Azure Synapse (SQL pools and data explorer)
  - Azure Data Lake Gen2 (blob containers: `bronze`, `silver`)

### ğŸ“¤ Sharing Scripts

To share the scripts hosted in Azure:
1. **Databricks notebooks**:  
   - Go to your workspace â†’ right-click notebook â†’ Export â†’ HTML or Source file (`.dbc` or `.ipynb`)
   - Add them to a `/notebooks` folder in this repo

2. **Data Factory Pipelines**:  
   - Use the "Export ARM Template" option from your Data Factory instance
   - Add the `ARMTemplateForFactory.json` file under `/infrastructure/` folder

3. **Synapse SQL Scripts**:  
   - Export your scripts from the Synapse workspace and include them in `/sql-scripts/`

---

## ğŸ§  Key Concepts

- **Delta Lake** & Parquet optimization
- `.join()` in PySpark
- `.write.mode()` usage: `append`, `overwrite`, `ignore`
- Serverless SQL querying
- End-to-end orchestration with Azure services

---

## ğŸš€ How to Run

1. Clone the repository:
   ```bash
   git clone [https://github.com/your-username/adventureworks-pipeline.git](https://github.com/HafsaOuaj/Azure-Data-Engineer-Project)
   ```

2. Load raw data into Azure Data Lake (Bronze layer).

3. Trigger ADF pipeline for ingestion.

4. Run Databricks notebooks for transformation.

5. Query data from Synapse or visualize with Power BI.

---

## ğŸ… Built By

**Hafsa Ouajdi**

- GitHub: [github.com/HafsaOuajdi](https://github.com/HafsaOuajdi)  
- Portfolio: [hafsaouaj.github.io/Portfolio_Hafsa](https://hafsaouaj.github.io/Portfolio_Hafsa)  
- Email: hafsa.ouajdi.fr@gmail.com  
```

